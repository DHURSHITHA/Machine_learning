ğŸ“˜ Naive Bayes Classifier (From Scratch â†’ Advanced)
1. Introduction

What is it?
Naive Bayes is a supervised learning algorithm based on Bayesâ€™ theorem.
It is called "naive" because it assumes that all features are independent of each other (which is rarely true in real life, but still works surprisingly well).

Purpose:
Given some input features, it predicts the probability of each possible class and chooses the class with the highest probability.

Applications:

Text Classification: Spam detection, sentiment analysis.

Medical Diagnosis: Predicting diseases from symptoms.

Recommendation Systems: Suggesting movies, books, etc.

âœ… Why important? Because it is fast, simple, and works well for text/categorical data.

2. Bayesâ€™ Theorem (The Core)

Bayesâ€™ theorem is a formula for finding the posterior probability:

ğ‘ƒ
(
ğ¶
âˆ£
ğ‘‹
)
=
ğ‘ƒ
(
ğ‘‹
âˆ£
ğ¶
)
â‹…
ğ‘ƒ
(
ğ¶
)
ğ‘ƒ
(
ğ‘‹
)
P(Câˆ£X)=
P(X)
P(Xâˆ£C)â‹…P(C)
	â€‹


Where:

P(C|X): Posterior â†’ Probability of class 
ğ¶
C given data 
ğ‘‹
X. (What we want to find!)

P(X|C): Likelihood â†’ Probability of seeing data 
ğ‘‹
X if the class is 
ğ¶
C.

P(C): Prior â†’ Probability of class 
ğ¶
C before seeing data.

P(X): Evidence â†’ Probability of the data overall (normalization factor).

ğŸ”‘ Intuition:
It updates our belief about the class after seeing the evidence.

3. The "Naive" Assumption

Normally, 
ğ‘ƒ
(
ğ‘‹
âˆ£
ğ¶
)
P(Xâˆ£C) could be very complex because 
ğ‘‹
X may have many features.
Naive Bayes assumes features are independent given the class.

That means:

ğ‘ƒ
(
ğ‘‹
âˆ£
ğ¶
)
=
ğ‘ƒ
(
ğ‘¥
1
âˆ£
ğ¶
)
â‹…
ğ‘ƒ
(
ğ‘¥
2
âˆ£
ğ¶
)
â‹…
â€¦
â‹…
ğ‘ƒ
(
ğ‘¥
ğ‘›
âˆ£
ğ¶
)
P(Xâˆ£C)=P(x
1
	â€‹

âˆ£C)â‹…P(x
2
	â€‹

âˆ£C)â‹…â€¦â‹…P(x
n
	â€‹

âˆ£C)

This simplifies computation drastically.

4. Types of Naive Bayes

Gaussian Naive Bayes

For continuous features.

Assumes features follow a Normal distribution.

Example: Predicting height/weight distribution of males vs females.

Multinomial Naive Bayes

For discrete features (like word counts).

Example: Text classification (word frequencies in emails).

Bernoulli Naive Bayes

For binary features (0/1 presence of word).

Example: Whether a word (â€œofferâ€) appears in spam or not.

5. Mathematical Formulation

Prior Probability (P(C))

Probability of each class in training data.

Example: 60% spam emails, 40% non-spam.

Likelihood (P(X|C))

Probability of feature values given class.

Example: Probability that word â€œfreeâ€ appears in spam.

Posterior (P(C|X))

Computed using Bayesâ€™ formula.

Decision Rule:

Choose class with highest posterior:

ğ¶
^
=
arg
â¡
max
â¡
ğ¶
ğ‘ƒ
(
ğ¶
âˆ£
ğ‘‹
)
C
^
=arg
C
max
	â€‹

P(Câˆ£X)
6. Implementation Steps
Data Preprocessing

Feature scaling: âŒ Not required.

Handle missing values: Use imputation.

Encode categorical variables: One-hot encoding.

Training

Use Scikit-learn (GaussianNB, MultinomialNB, BernoulliNB).

Fit with training data.

Prediction

Use .predict() to classify new data.

Hyperparameters

Smoothing (Laplace smoothing / Î±):
Avoids zero probability for unseen words/features.
(Very important in text classification).

7. Evaluation Metrics

Accuracy â†’ % of correct predictions.

Precision â†’ Of predicted positives, how many are correct?

Recall â†’ Of actual positives, how many did we catch?

F1-score â†’ Balance between precision & recall.

Confusion Matrix â†’ Table of TP, FP, TN, FN.

âœ… Use cross-validation (like k-fold) to ensure generalization.

8. Advantages

Simple, easy, and fast.

Works well with large datasets and high-dimensional data (like text).

Handles categorical features very well.

Can work with missing/irrelevant features.

9. Limitations

Independence assumption is often false in real-world data.

Doesnâ€™t perform well when features are correlated.

Less powerful than advanced models (e.g., Random Forest, Neural Networks).

10. Practical Considerations

Imbalanced Data: Adjust priors or thresholds.

Feature Engineering: Choose useful features, remove noisy ones.

Interpretability: Easy to explain because it gives probabilities.

11. Real-World Applications

Text Classification: Spam filtering, sentiment analysis.

Healthcare: Disease prediction from symptoms.

Finance: Fraud detection, credit risk scoring.

12. Conclusion

Naive Bayes is based on Bayesâ€™ theorem + independence assumption.

It is simple yet powerful, especially in text classification.

Works best for categorical data and situations where speed is important.

Limitations: Doesnâ€™t capture correlations well, so not always best for complex data.

âœ… Exam Tip: Always write:

Definition + Formula.

Assumptions.

Types (Gaussian, Multinomial, Bernoulli).

Applications.

Advantages & Limitations.

Example (like spam filtering).